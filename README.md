# Full-Stack LLM Agent with Dynamic Knowledge Base

A production-ready web application that showcases an end-to-end AI product development pipeline. This application allows users to chat with a RAG-powered LLM agent and dynamically update its knowledge base by uploading new documents.

## ğŸ¯ Project Overview

This project demonstrates:
- **Backend API**: FastAPI with RAG pipeline using LangChain, ChromaDB, and OpenAI
- **Frontend**: Modern React application with chat interface and document upload
- **Deployment**: Docker containerization with docker-compose for local development
- **AI Integration**: Retrieval-Augmented Generation (RAG) with dynamic knowledge base updates

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React Frontendâ”‚    â”‚  FastAPI Backendâ”‚    â”‚   ChromaDB      â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚   Vector Store  â”‚
â”‚  - Chat Interfaceâ”‚â—„â”€â”€â–ºâ”‚  - RAG Pipeline â”‚â—„â”€â”€â–ºâ”‚                 â”‚
â”‚  - File Upload  â”‚    â”‚  - Document Procâ”‚    â”‚  - Embeddings   â”‚
â”‚  - Real-time UI â”‚    â”‚  - OpenAI API   â”‚    â”‚  - Persistence  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Features

### Chat Interface
- Real-time conversation with the AI agent
- Source attribution showing which documents were used
- Responsive design with modern UI/UX
- Streaming responses (optional)

### Document Upload
- Drag-and-drop PDF upload interface
- Automatic text extraction and chunking
- Vector embedding generation
- Dynamic knowledge base updates

### Backend API
- RESTful API with FastAPI
- Automatic API documentation
- Error handling and validation
- Health check endpoints

## ğŸ“‹ Prerequisites

- Python 3.9+
- Node.js 18+
- Docker and Docker Compose
- OpenAI API key

## ğŸ› ï¸ Installation & Setup

### 1. Clone the Repository
```bash
git clone <repository-url>
cd full-stack-llm-agent
```

### 2. Environment Setup
Create a `.env` file in the backend directory:
```bash
cp backend/env_example.txt backend/.env
```

Edit `backend/.env` and add your OpenAI API key:
```
OPENAI_API_KEY=your_openai_api_key_here
CHROMA_PERSIST_DIRECTORY=./chroma_db
```

### 3. Local Development

#### Option A: Using Docker Compose (Recommended)
```bash
# Start all services
docker-compose up --build

# The application will be available at:
# Frontend: http://localhost:3000
# Backend API: http://localhost:8000
# API Docs: http://localhost:8000/docs
```

#### Option B: Manual Setup

**Backend:**
```bash
cd backend
pip install -r requirements.txt
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

**Frontend:**
```bash
cd frontend
npm install
npm start
```

## ğŸ“š API Endpoints

### Chat Endpoints
- `POST /chat` - Send a message to the AI agent
- `POST /chat/stream` - Stream chat response (Server-Sent Events)

### Document Management
- `POST /upload` - Upload a new document to the knowledge base
- `GET /documents` - List all uploaded documents

### Health & Status
- `GET /` - Basic health check
- `GET /health` - Detailed health status

## ğŸ§ª Usage

1. **Start the application** using Docker Compose or manual setup
2. **Open the frontend** at http://localhost:3000
3. **Upload documents** using the Upload tab to add knowledge to the agent
4. **Chat with the agent** using the Chat tab to ask questions about uploaded documents

### Example Workflow
1. Upload a PDF document about machine learning
2. Ask: "What is machine learning?"
3. The agent will answer based on the uploaded document content
4. Upload more documents to expand the knowledge base
5. Ask follow-up questions that may require information from multiple documents

## ğŸ—ï¸ Project Structure

```
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                 # FastAPI application
â”‚   â”œâ”€â”€ rag_pipeline.py         # RAG implementation
â”‚   â”œâ”€â”€ document_processor.py   # Document processing logic
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile             # Backend container
â”‚   â””â”€â”€ chroma_db/             # Vector database storage
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/        # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ Header.js
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInterface.js
â”‚   â”‚   â”‚   â””â”€â”€ DocumentUpload.js
â”‚   â”‚   â”œâ”€â”€ App.js             # Main app component
â”‚   â”‚   â””â”€â”€ index.js           # App entry point
â”‚   â”œâ”€â”€ package.json           # Node.js dependencies
â”‚   â”œâ”€â”€ Dockerfile            # Frontend container
â”‚   â””â”€â”€ nginx.conf            # Nginx configuration
â”œâ”€â”€ docker-compose.yml         # Multi-container setup
â””â”€â”€ README.md                 # This file
```

## ğŸ”§ Configuration

### Environment Variables
- `OPENAI_API_KEY`: Your OpenAI API key (required)
- `CHROMA_PERSIST_DIRECTORY`: Path for ChromaDB storage (default: ./chroma_db)

### Customization
- **Chunk Size**: Modify `chunk_size` in `rag_pipeline.py` and `document_processor.py`
- **Embedding Model**: Change the model in `rag_pipeline.py`
- **UI Theme**: Customize colors and styles in `App.css`

## ğŸš€ Deployment

### AWS Deployment (Recommended)
1. **Build and push Docker images** to AWS ECR
2. **Deploy using AWS App Runner** or **Elastic Beanstalk**
3. **Set environment variables** in the deployment configuration
4. **Configure domain and SSL** if needed

### Other Cloud Providers
- **Google Cloud Run**: Deploy containers directly
- **Azure Container Instances**: Similar to AWS App Runner
- **Heroku**: Use container deployment

## ğŸ§ª Testing

### Backend Testing
```bash
cd backend
python -m pytest tests/
```

### Frontend Testing
```bash
cd frontend
npm test
```

### API Testing
Use the interactive API documentation at http://localhost:8000/docs

## ğŸ“ˆ Performance Considerations

- **Chunk Size**: Optimize for your document types (default: 1000 characters)
- **Embedding Model**: Consider using more powerful models for better accuracy
- **Caching**: Implement Redis for frequently accessed embeddings
- **Scaling**: Use load balancers and multiple backend instances

## ğŸ”’ Security Considerations

- **API Keys**: Store securely using environment variables
- **File Upload**: Validate file types and sizes
- **Rate Limiting**: Implement rate limiting for API endpoints
- **CORS**: Configure CORS properly for production

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Troubleshooting

### Common Issues

1. **OpenAI API Key Error**
   - Ensure your API key is correctly set in the `.env` file
   - Check that you have sufficient API credits

2. **ChromaDB Connection Issues**
   - Ensure the `chroma_db` directory has proper permissions
   - Try deleting the directory to reset the database

3. **Frontend Not Loading**
   - Check that the backend is running on port 8000
   - Verify CORS settings in the backend

4. **Document Upload Fails**
   - Ensure the file is a valid PDF
   - Check file size limits (10MB default)

### Getting Help
- Check the logs: `docker-compose logs backend` or `docker-compose logs frontend`
- Review the API documentation at http://localhost:8000/docs
- Open an issue on GitHub for bugs or feature requests

## ğŸ‰ Success Criteria

âœ… **Deployed Application**: Accessible via public URL  
âœ… **Interactive Chat**: Users can engage with the RAG agent  
âœ… **Dynamic Upload**: Users can upload documents and expand knowledge  
âœ… **Containerized**: Full Docker setup with docker-compose  
âœ… **Production Ready**: Error handling, logging, and monitoring  

This project successfully demonstrates modern AI product development with a complete full-stack implementation!
